{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b188ad6e",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaef483",
   "metadata": {},
   "source": [
    "1、分类任务 损失函数 \n",
    "  - 二分类 ： 二分类交叉熵损失函数  Binary Cross-Entropy \n",
    "  - 多分类 ： 多分类交叉熵损失函数  Categorical Cross-Entropy \n",
    "  - Focal Loss ： 优化版的交叉熵损失函数， 用于解决难\\易分类样本数量不均衡的问题  \n",
    "\n",
    "2、回归任务 损失函数\n",
    "  - 均方误差 MSE （Mean Squared Error）\n",
    "  - 平均绝对误差 MAE （Mean Absolute Error）\n",
    "\n",
    "3、目标检测任务 损失函数\n",
    "\n",
    "目标检测任务的损失函数由 类别损失（Classificition Loss） 和 定位损失（Bounding Box Regeression Loss） 两部分构成。\n",
    "- 类别损失（分类损失） ： \n",
    "  - 交叉熵损失 \n",
    "  - Focal Loss\n",
    "- 定位损失 (回归损失)：\n",
    "  - SmoothL1 Loss   \n",
    "  - IoU Loss  \n",
    "  - GIoU Loss \n",
    "  - DIoU Loss \n",
    "  - CIoU Loss \n",
    "  - EIoU Loss \n",
    "  - Alpha IoU Loss \n",
    "  - SIoU Loss \n",
    "  - WIoU Loss \n",
    "  - Shape-IoU  Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b3c9a",
   "metadata": {},
   "source": [
    "## 二元交叉熵损失函数 Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3de40a",
   "metadata": {},
   "source": [
    "1、二元交叉熵损失函数\n",
    "\n",
    "（1）二元交叉熵损失函数 （Binary Cross Entropy Loss） 适用于二分类问题 ： 样本标签为二元值：0 或 1\n",
    "\n",
    "（2）用于将 模型 预测值 和 真实标签值 之间的差异转化为一个标量值，从而衡量模型预测的准确性。\n",
    "          计算公式 ：\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y_i}) + (1-y_i) \\log(1-\\hat{y_i})]$$\n",
    "其中：\n",
    "- $N$ 表示样本数量\n",
    "- $y_i$ 表示第 $i$个样本的真实标签\n",
    "- $\\hat{y_i}$ 表示第 $i$个样本的预测值\n",
    "\n",
    "如果 \n",
    "$y_i=1$，则第一项 $y_i \\log(\\hat{y_i})$ 生效，第二项 $(1-y_i) \\log(1-\\hat{y_i})$ 失效\n",
    "\n",
    "如果 $y_i=0$，则第一项 $y_i \\log(\\hat{y_i})$ 失效，第二项 $(1-y_i) \\log(1-\\hat{y_i})$ 生效\n",
    "\n",
    "### 2、 nn.BCELoss() 类\n",
    "\n",
    "`nn.BCELoss()` 是pytorch 实现的二元交叉熵损失函数，也称为对数损失函数（Log Loss）\n",
    "\n",
    "```python\n",
    "torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "参数说明：\n",
    "  - weight ：用于样本加权的权重张量。如果给定，则必须是一维张量，大小等于输入张量的大小。默认值为 None。\n",
    "  - reduction ：指定如何计算损失值。可选值为 'none'、'mean' 或 'sum'。默认值为 'mean'\n",
    "\n",
    "### 3、使用场景举例\n",
    "\n",
    "假设有一个 二分类任务：判断图片中是否包含猫。\n",
    "\n",
    "该图像的标签值为  0 或 1\n",
    "\n",
    "我们可以定义一个二元分类模型，用 Sigmoid 输出一个概率值，表示样本属于猫的概率。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CatClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CatClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(5, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CatClassifier()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "x = torch.rand((3, 5))\n",
    "label = torch.tensor([0, 1, 1], dtype=torch.float32)\n",
    "pred = model(x)  # tensor([[0.6140],[0.5350],[0.5852]], grad_fn=<SigmoidBackward0>)\n",
    "loss = criterion(pred.squeeze(), label)\n",
    "print(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adca364",
   "metadata": {},
   "source": [
    "### 4、torch.nn.BCEWithLogitsLoss()  与 nn.BCELoss() 的区别\n",
    "\n",
    "`nn.BCELoss()`  的输入是 二元分类模型的预测值 $\\hat{y}$ 和 实际标签 $y$。并且$\\hat{y}$的范围是 [0,1]，因为二元分类模型内部已经对预测结果做了 sigmoid 处理。\n",
    "\n",
    "$$nn.BCELoss()=-\\frac{1}{N}\\sum_{i=1}^N[y_i\\log(\\hat{y_i})+(1-y_i)\\log(1-\\hat{y_i})]$$\n",
    "\n",
    "`torch.nn.BCEWithLogitsLoss()` 的输入也是 二元分类模型的输出值 $z$ 和实际标签 $y$，不同的是输出$z$在模型内部没有经过 sigmoid 处理，是任意实数。  这种情况下，sigmoid 处理就被放到了损失函数中，\n",
    "\n",
    "所以，`torch.nn.BCEWithLogitsLoss()`  函数内部的计算过程是先对 $z$ 应用 sigmoid 函数，将其映射到 [0,1] 范围内，然后再使用二元交叉熵 计算预测值和实际标签之间的损失值。\n",
    "\n",
    "$$nn.BCEWithLogitsLoss()=-\\frac{1}{N}\\sum_{i=1}^N[y_i\\log\\sigma(z_i)+(1-y_i)\\log(1-\\sigma(z_i))]$$\n",
    "\n",
    "另外，`torch.nn.BCEWithLogitsLoss() `还支持设置 pos_weight 参数，用于处理样本不平衡的问题。而 `nn.BCELoss()` 不支持设置 pos_weight 参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19337f",
   "metadata": {},
   "source": [
    "### 5、torch.nn.BCEWithLogitsLoss() 类\n",
    "\n",
    "```python\n",
    "torch.nn.BCEWithLogitsLoss(weight=None,\n",
    "                           size_average=None,                            \n",
    "                           reduce=None, \n",
    "                           reduction='mean',     \n",
    "                           pos_weight=None)\n",
    "```\n",
    "\n",
    "参数：\n",
    "  - weight：用于对每个样本的损失值进行加权。默认值为 None。\n",
    "  - reduction：指定如何对每个 batch 的损失值进行降维。可选值为 ‘none’、‘mean’ 和 ‘sum’。默认值为 ‘mean’。\n",
    "  - pos_weight：用于对正样本的损失值进行加权。可以用于处理样本不平衡的问题。例如，如果正样本比负样本少很多，可以设置 pos_weight 为一个较大的值，以提高正样本的权重。默认值为 None。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b433d2",
   "metadata": {},
   "source": [
    "## 交叉熵损失函数 Cross Entropy Loss\n",
    "\n",
    "### 1、交叉熵损失函数\n",
    "\n",
    "交叉熵损失多用于 多分类任务，下面我们通过拆解交叉熵的公式来理解其作为损失函数的意义\n",
    "\n",
    "假设我们在做一个  n分类的问题，模型预测的输出结果是 $[x_1,  x_2, x_3, ...., x_n]$  \n",
    "然后，我们选择交叉熵损失函数作为目标函数，通过反向传播调整模型的权重\n",
    "\n",
    "交叉熵损失函数的公式 ：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "loss(x,class) & =-log(\\frac{e^{x_{[class]}}}{\\sum_je^{x_j}}) \\\\\n",
    " & =-x_{[class]}+log(\\sum_je^{x_j})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $x$是预测结果，是一个向量 $x=[x_1, x_2, x_3, ...., x_n]$，其元素个数 和 类别数一样多\n",
    "- class 表示这个样本的实际标签，比如，样本实际属于分类 2，那么 $class=2$  ， $x_{[class]}$ 就是$x_2$，就是取预测结果向量中的第二个元素，即，取其真实分类对应的那个类别的预测值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23d331",
   "metadata": {},
   "source": [
    "1. 首先，交叉熵损失函数中包含了一个最基础的部分：$softmax(x_i)=\\frac{e^{x_i}}{\\sum_{j=0}^ne^{x_j}}$\n",
    "    softmax 将分类的结果做了归一化：\n",
    "    - 先经过 $e^{x}$ 的运算，将 $x$ 转换为非负数\n",
    "    - 再通过公式 $\\frac{e^{x_i}}{\\sum_{j=0}^ne^{x_j}}$ 计算出该样本被分到分类$i$概率，这里所有分类概率相加的总和等于1\n",
    "2. 我们想要使预测结果中，真实分类的那个概率接近 100%。 我们取出真实分类的那个概率：\n",
    "  $\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}}$，我们希望它的值是 100%\n",
    "3.  作为损失函数，后面需要参与求导。乘\\除法 表达式求导比较麻烦，所以最好想办法转化成加\\减法表达式。最自然的想法是取对数，把乘\\除法转化为加\\减法表达式：\n",
    "\n",
    "$log{\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}}} = log{e^{x_{[class]}}} - log{\\sum_{j=0}^{n} e^{x_{j}}}$\n",
    "\n",
    "  - 由于对数单调增，那么求 $\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}}$ 的最大值的问题，可以转化为求 $log{\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}}}$ 的最大值的问题。\n",
    "  \n",
    "  -  $\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}}$ 的取值范围是 (0, 1)，最大值为1。 取对数之后，$log{\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}}}$ 的取值范围为 [−∞,0]，最大值为0\n",
    "\n",
    "\n",
    "4. 作为损失函数的意义是：当预测结果越接近真实值，损失函数的值越接近于0\n",
    "\n",
    "  所以，我们把 $log{\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}}}$ 取反之后，$-log{\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}}}$  最小值为0\n",
    "  \n",
    "  这样就能保证当  $\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}}$ 越接近于 100%， $loss=-log(\\frac{e^{x_{[class]}}}{\\sum_{j=0}^{n} e^{x_{j}}})$  越接近0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8668fc",
   "metadata": {},
   "source": [
    "### 2、nn.CrossEntropyLoss\n",
    "\n",
    "```python\n",
    "nn.CrossEntropyLoss(weight=None, \n",
    "                    reduction='mean'，\n",
    "                    ignore_index=-100)\n",
    "```\n",
    "\n",
    "参数 ：\n",
    "- weight (optional): 一个张量，用于为每个类别的 loss 设置权值。可以用于处理类别不平衡的情况。\n",
    "  - 默认值为None\n",
    "  - weight必须是float类型的 tensor，其长度要与类别个数一致，即每一个类别都要设置权重值\n",
    "  $loss(x, class) = weight_{[class]}(-log(\\frac{e^{x_{[class]}}}{\\sum_je^{x_{j}}}))$\n",
    "- reduction (string, optional): 指定损失的计算方式，可选值有：\"none\"、\"mean\"、\"sum\"\n",
    "  - \"none\" ：表示不进行任何降维，返回每个样本的损失\n",
    "  - \"mean\"：表示对参与计算的样本的损失取平均值，（ \"mean\" 为默认值）\n",
    "  - \"sum\"：表示对参与计算的样本的损失求和\n",
    "- ignore_index (int, optional): 忽略目标中的特定类别索引，不计入损失计算。默认值为-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1336b858",
   "metadata": {},
   "source": [
    "### 3、应用\n",
    "\n",
    "假设有4张图片( batch_ size=4) \n",
    "我们需要把这4张图片分类到5个类别上去，比如说：鸟，狗，猫，汽车，船\n",
    "经过网络得到的预测结果为：predict，size=[4, 5]\n",
    "其真实标签为 label，size=[4]\n",
    "\n",
    "接下来使用 `nn.CrossEntropyLoss()`计算 预测结果 predict 和 真实值label 的交叉熵损失\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------------------\n",
    "# 定义数据: batch_size=4；  一共有5个分类\n",
    "# label.size() : torch.Size([4])\n",
    "# predict.size(): torch.Size([4, 5])\n",
    "# -----------------------------------------\n",
    "torch.manual_seed(100)\n",
    "predict = torch.rand(4, 5)\n",
    "label = torch.tensor([4, 3, 3, 2])\n",
    "print(predict)\n",
    "print(label)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 直接调用函数 nn.CrossEntropyLoss() 计算 Loss\n",
    "# -----------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(predict, label)\n",
    "print(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41309c",
   "metadata": {},
   "source": [
    "## 信息量、熵、交叉熵、相对熵\\KL散度、交叉熵损失函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d1ea8",
   "metadata": {},
   "source": [
    "### 1、信息量 Amount of Information\n",
    "\n",
    "![image](/home/vtchen/00-coding/01-deep_learning/02-Loss_fun/images/1.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f08ce",
   "metadata": {},
   "source": [
    "### 2、熵 Entropy\n",
    "\n",
    "![image.png](/home/vtchen/00-coding/01-deep_learning/02-Loss_fun/images/2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76b5d1",
   "metadata": {},
   "source": [
    "### 3、交叉熵 Cross Entropy\n",
    "\n",
    "![image.png](/home/vtchen/00-coding/01-deep_learning/02-Loss_fun/images/3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c06659",
   "metadata": {},
   "source": [
    "### 4、相对熵 Relative Entropy \\ KL散度  KLDivergence\n",
    "![image](/home/vtchen/00-coding/01-deep_learning/02-Loss_fun/images/4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e0c16",
   "metadata": {},
   "source": [
    "### 5、交叉熵损失函数 Cross Entropy\n",
    "![image](/home/vtchen/00-coding/01-deep_learning/02-Loss_fun/images/5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48741790",
   "metadata": {},
   "source": [
    "## 均方误差损失函数 MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031685a",
   "metadata": {},
   "source": [
    "### 1、均方差损失 MSE\n",
    "均方误差（ MSE ：Mean Squared Error ）损失函数，也称为平方损失函数，是用于回归任务的一种常见损失函数。\n",
    "该损失函数 用于衡量 模型对于每个样本预测值与实际值之间的平方差异的平均程度。\n",
    "\n",
    "均方误差损失函数 的数学表达式为：\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i-\\hat y_i)^2$$\n",
    "\n",
    "\n",
    "- 其中：\n",
    "  * $n$ 是样本数量\n",
    "  * $y_i$ 是第 $i$个样本的 真实标签\n",
    "  * $\\hat{y_i}$ 是第 $i$个样本的 预测值\n",
    "\n",
    "- 均方误差损失函数 对差异的平方进行了求和，这使得较大的误差在计算中得到了更大的权重，这也导致它对离群值敏感，因为它会放大离群值的影响。\n",
    "- 均方误差损失函数 在回归问题中广泛使用，尤其在对误差敏感度较高的情况下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997e5be",
   "metadata": {},
   "source": [
    "### 2、 torch.nn.MSELoss() \n",
    "```python\n",
    "torch.nn.MSELoss(reduction='mean')\n",
    "```\n",
    "参数 reduction ：  指定损失的计算方式， 可选值有：\"none\"、\"mean\"、\"sum\"\n",
    "- reduction='None' ：按照样本原始维度输出，不做另外处理\n",
    "- reduction='mean'  (默认)：对应位置求和后取平均\n",
    "- reduction='sum'，：对应位置求和\n",
    "\n",
    "### 3、应用\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个示例的模型输出和真实标签\n",
    "torch.manual_seed(121)\n",
    "model_output = torch.randn(10, requires_grad=True)  # 模型的输出\n",
    "true_labels = torch.randn(10)  # 真实标签\n",
    "\n",
    "# 创建均方误差损失函数\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# 计算损失\n",
    "loss = mse_loss(model_output, true_labels)\n",
    "\n",
    "# 打印损失值\n",
    "print(\"Mean Squared Error Loss:\", loss.item())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f72250",
   "metadata": {},
   "source": [
    "## 平均绝对误差损失函数 MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9092c",
   "metadata": {},
   "source": [
    "### 1、平均绝对误差损失函数 MAE \n",
    "平均绝对误差损失函数（MAE ：Mean Absolute Error），也称为 绝对值损失函数，通常用于回归任务\n",
    "\n",
    "该损失函数 用于衡量 模型对于每个样本预测值与实际值之间的平方差异的平均程度。\n",
    "\n",
    "平均绝对误差 损失函数的数学表达式为：\n",
    "$$MAE = \\frac{1}{n} \\sum^n_{i=1}｜y_i-\\hat y_i｜$$\n",
    "其中：\n",
    "  - $n$ 是样本数量\n",
    "  - $y_i$ 是第 $i$个样本的 真实标签\n",
    "  - $\\hat{y_i}$ 是第 $i$个样本的 预测值\n",
    "\n",
    "- 相比于均方误差 (MSE)，绝对值损失函数 (MAE) 对离群值不敏感，因为它不会对差异值求平方\n",
    "- 平均绝对损失函数适用于回归任务，特别是当数据中存在离群值或对预测误差的敏感性要求较低的情况下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dccb89",
   "metadata": {},
   "source": [
    "### 2、 torch.nn.L1Loss()\n",
    "```python\n",
    "torch.nn.L1Loss(reduction='mean')\n",
    "```\n",
    "参数 reduction ：  指定损失的计算方式， 可选值有：\"none\"、\"mean\"、\"sum\"\n",
    "- reduction='None' ：按照样本原始维度输出，不做另外处理\n",
    "- reduction='mean'  (默认)：对应位置求和后取平均\n",
    "- reduction='sum'，：对应位置求和"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29706fc",
   "metadata": {},
   "source": [
    "### 3、应用\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个示例输入和目标\n",
    "torch.manual_seed(121)\n",
    "predictions = torch.randn((3,))  # 模型的预测值\n",
    "targets = torch.randn((3,))      # 实际目标值\n",
    "\n",
    "# 创建 MAE 损失函数实例\n",
    "mae_loss = nn.L1Loss()\n",
    "\n",
    "# 计算损失\n",
    "loss = mae_loss(predictions, targets)\n",
    "\n",
    "# 打印结果\n",
    "print('MAE Loss:', loss.item())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2183cf0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aad8acea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mojo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
